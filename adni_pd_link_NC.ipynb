{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f52f08f-c577-43e3-9827-202ac1253654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire, sys, tqdm\n",
    "import os\n",
    "import yaml\n",
    "import cv2\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from skimage import io as skio\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "import transformers as tf\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torchvision import transforms\n",
    "\n",
    "import utils.load as load\n",
    "from rgcn import RGCN\n",
    "from utils.load import tic, toc, here\n",
    "from model_utils import sum_sparse, adj, pca, enrich, bert_emb, squeeze_emb, score_distmult_bc, compute_ranks_fast, sfcn_emb\n",
    "from squeezenet import SqueezeNetwork\n",
    "from SFCNnet import SFCNNetwork\n",
    "from utils.data_utils import PrepareDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6dd0a8c-774f-4396-accb-5ddc50395a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_classification_task(config_yaml, data, emb, trainable, device):\n",
    "    \n",
    "    '''\n",
    "    with early stopping. save model with lowest validation loss\n",
    "    '''\n",
    "    final = config_yaml[\"data_prep\"][\"final\"]\n",
    "    split_ratio = config_yaml[\"link_pred\"][\"split_ratio\"]\n",
    "    bases = config_yaml[\"rgcn\"][\"bases\"]\n",
    "    lr = config_yaml[\"rgcn\"][\"lr\"]\n",
    "    l2 = config_yaml[\"rgcn\"][\"l2\"]\n",
    "    wd = config_yaml[\"rgcn\"][\"wd\"]\n",
    "    epochs = config_yaml[\"training\"][\"epochs\"]\n",
    "    printnorms = config_yaml[\"training\"][\"printnorms\"]\n",
    "    patience = config_yaml[\"training\"][\"patience\"]\n",
    "    best_score = config_yaml[\"training\"][\"best_score\"]\n",
    "    delta = config_yaml[\"training\"][\"delta\"]\n",
    "    model_path = config_yaml[\"training\"][\"model_path\"]\n",
    "    results_path = config_yaml[\"training\"][\"results_path\"]\n",
    "    \n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    training_acc_list = []\n",
    "    withheld_acc_list = []\n",
    "\n",
    "    results_dict = {}\n",
    "    \n",
    "    rgcn = RGCN(data.triples.long().to(device), n=data.num_entities, r=data.num_relations, insize=emb, hidden=emb, numcls=data.num_classes, device = device, link_prediction = False, bases=bases)\n",
    "\n",
    "    rgcn.to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(lr=lr, weight_decay=wd, params=rgcn.parameters())\n",
    "    \n",
    "    if final==False:\n",
    "        patience_left = patience\n",
    "        best_state = None\n",
    "        for e in range(epochs):\n",
    "            tic()\n",
    "            # both the train & validation are already on graph. But only the loss from train is used to backpropagate and learn the graph\n",
    "\n",
    "            # https://github.com/dmlc/dgl/blob/master/examples/pytorch/rgcn/entity_classify.py\n",
    "            opt.zero_grad()\n",
    "            # features are all the data\n",
    "            features = torch.cat([trainable], dim=0) # features: number of entities in dataset x embedding size\n",
    "            features = features.requires_grad_()\n",
    "            features.to(device)\n",
    "            out = rgcn(features)\n",
    "\n",
    "            idxt, clst = data.training[:, 0], data.training[:, 1] # indices, classes\n",
    "            idxw, clsw = data.withheld[:, 0], data.withheld[:, 1] # indices, classes\n",
    "\n",
    "            clst = clst.to(device)\n",
    "            clsw = clsw.to(device)\n",
    "            out_train = out[idxt, :]\n",
    "            loss = F.cross_entropy(out_train, clst, reduction='mean')\n",
    "            val_loss = F.cross_entropy(out[idxw,:], clsw, reduction='mean')\n",
    "            if l2 != 0.0:\n",
    "                loss = loss + l2 * rgcn.penalty()\n",
    "\n",
    "            # compute performance metrics\n",
    "            with torch.no_grad():\n",
    "                # clst & clsw are class labels\n",
    "                # check if the probabilities given out given instance \n",
    "                training_acc = (out[idxt, :].argmax(dim=1) == clst).sum().item() / idxt.size(0)\n",
    "                withheld_acc = (out[idxw, :].argmax(dim=1) == clsw).sum().item() / idxw.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epochs_list.append(e)\n",
    "            train_loss_list.append(loss.cpu().detach().numpy())\n",
    "            valid_loss_list.append(val_loss.cpu().detach().numpy())\n",
    "            training_acc_list.append(training_acc)\n",
    "            withheld_acc_list.append(withheld_acc)\n",
    "\n",
    "            if e> 20:\n",
    "                if best_score < 0:\n",
    "                    best_score = val_loss\n",
    "                    val_acc = withheld_acc\n",
    "                    model_dict = OrderedDict()\n",
    "                    for k, v in rgcn.state_dict().items():\n",
    "                        if k not in ['hor_graph','ver_graph']:\n",
    "                            model_dict[k] = v \n",
    "\n",
    "                    best_state = model_dict\n",
    "                if val_loss >= best_score - delta: # if validation loss is greater than best score\n",
    "                    patience_left -= 1\n",
    "                else:\n",
    "                    best_score = val_loss\n",
    "                    val_acc = withheld_acc\n",
    "                    model_dict = OrderedDict()\n",
    "                    for k, v in rgcn.state_dict().items():\n",
    "                        if k not in ['hor_graph','ver_graph']: # cannot save sparse tensors\n",
    "                            model_dict[k] = v \n",
    "\n",
    "                    best_state = model_dict\n",
    "                    patience_left = patience\n",
    "                if patience_left <= 0:\n",
    "                    torch.save(best_state, f'{model_path}/nc_{val_acc:.2}.pt')\n",
    "                    print(\"Early stopping after no improvement for {} epoch\".format(patience))\n",
    "                    break\n",
    "\n",
    "            print(f'epoch {e:02}: loss {loss:.2}, train acc {training_acc:.2}, \\t val_loss {val_loss:.2}, withheld acc {withheld_acc:.2} \\t ({toc():.5}s)')\n",
    "\n",
    "            results_dict = {'epochs':epochs_list, 'train_loss': train_loss_list, 'valid_loss': valid_loss_list, 'training_acc':training_acc_list, 'validation_acc': withheld_acc_list}\n",
    "            df = pd.DataFrame(data=results_dict)\n",
    "            df.to_csv(os.path.join(results_path, \"nc_train_val_logs.csv\"), index=False)\n",
    "            \n",
    "    elif final==True:\n",
    "        features = torch.cat([trainable], dim=0)\n",
    "        rgcn = RGCN(data.triples.long().to(device), n=data.num_entities, r=data.num_relations, insize=emb, hidden=emb, numcls=data.num_classes, device = device, link_prediction = False, bases=bases)\n",
    "        rgcn.to(device)\n",
    "        rgcn.load_state_dict(torch.load(f'{model_path}/nc_0.66.pt'),strict=False)\n",
    "        \n",
    "        test_results_dict = {}\n",
    "        \n",
    "        rgcn.eval()\n",
    "        features.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = rgcn(features)\n",
    "\n",
    "        idxw, clsw = data.withheld[:, 0], data.withheld[:, 1] # indices, classes\n",
    "        clsw = clsw.to(device)\n",
    "        out_test = out[idxw, :]\n",
    "        test_acc = (out[idxw, :].argmax(dim=1) == clsw).sum().item() / idxw.size(0)\n",
    "        # scores on set\n",
    "        test_results_dict = {'type':'test', 'acc':test_acc}\n",
    "        with open(os.path.join(results_path, \"nc_test_logs.csv\"), 'w') as file:\n",
    "             file.write(json.dumps(test_results_dict)) # use `json.loads` to do the reverse\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fd82f7-eee0-4c50-ae74-22f7d9f00bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_prediction_task(config_yaml, data, emb, trainable, device):\n",
    "    \n",
    "    '''\n",
    "    evaluate on training & validation, then immediate evaluate on test.\n",
    "    \n",
    "    with early stopping\n",
    "    '''\n",
    "    \n",
    "    results_path = config_yaml[\"training\"][\"results_path\"]\n",
    "    split_ratio = config_yaml[\"link_pred\"][\"split_ratio\"]\n",
    "    bases = config_yaml[\"rgcn\"][\"bases\"]\n",
    "    lr = config_yaml[\"rgcn\"][\"lr\"]\n",
    "    l2 = config_yaml[\"rgcn\"][\"l2\"]\n",
    "    wd = config_yaml[\"rgcn\"][\"wd\"]\n",
    "    epochs = config_yaml[\"training\"][\"epochs\"]\n",
    "    final = config_yaml[\"data_prep\"][\"final\"]\n",
    "    patience = config_yaml[\"training\"][\"patience\"]\n",
    "    best_score = config_yaml[\"training\"][\"best_score\"]\n",
    "    delta = config_yaml[\"training\"][\"delta\"]\n",
    "    model_path = config_yaml[\"training\"][\"model_path\"]\n",
    "    \n",
    "    results_df = pd.DataFrame()\n",
    "    results_dict = {}\n",
    "    \n",
    "    train_val_triples, test_triples = train_test_split(data.triples.long(), test_size = split_ratio, random_state=0)\n",
    "    train_triples, val_triples = train_test_split(train_val_triples, test_size = split_ratio, random_state=0)\n",
    "    rgcn = RGCN(data.triples.long().to(device), n=data.num_entities, r=data.num_relations, insize=emb, hidden=emb, numcls=data.num_classes, device = device, link_prediction=True, bases=bases)\n",
    "    rgcn.to(device)\n",
    "    \n",
    "    opt = torch.optim.Adam(lr=lr, weight_decay=wd, params=rgcn.parameters())\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    valid_MRR_list = []\n",
    "    hits1_list = []\n",
    "    hits3_list = []\n",
    "    hits10_list = []\n",
    "    hits100_list = []\n",
    "\n",
    "    \n",
    "    patience_left = patience\n",
    "    best_state = None\n",
    "    for e in range(epochs):\n",
    "        tic()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        rgcn.train()\n",
    "        features = torch.cat([trainable], dim=0)\n",
    "        features.requires_grad_()\n",
    "        node_embeddings = rgcn(features) # number of entities x embedding size => NEED TO CHECK IF CAN PASS THE WHOLE FEATURE MATRIX IN OR JUST THE TRAIN MATRIX?\n",
    "        edge_embeddings = rgcn.w_relation\n",
    "\n",
    "        nsamples = len(train_triples)\n",
    "        ncorrupt = nsamples//5\n",
    "        neg_samples_idx = torch.from_numpy(np.random.choice(np.arange(nsamples),\n",
    "                                                            ncorrupt,\n",
    "                                                            replace=False))\n",
    "\n",
    "        # creating corrupted triples\n",
    "\n",
    "        ncorrupt_head = ncorrupt//2\n",
    "        ncorrupt_tail = ncorrupt - ncorrupt_head\n",
    "        corrupted_data = torch.empty((ncorrupt, 3), dtype=torch.int64)\n",
    "\n",
    "        corrupted_data = train_triples[neg_samples_idx]\n",
    "        corrupted_data[:ncorrupt_head, 0] = torch.from_numpy(np.random.choice(np.arange(data.num_entities),\n",
    "                                                                              ncorrupt_head))\n",
    "        corrupted_data[-ncorrupt_tail:, 2] = torch.from_numpy(np.random.choice(np.arange(data.num_entities),\n",
    "                                                                               ncorrupt_tail))\n",
    "\n",
    "\n",
    "        # compute score\n",
    "        Y = torch.ones(nsamples+ncorrupt, dtype=torch.float32)\n",
    "        Y[-ncorrupt:] = 0\n",
    "        Y = Y.to(device)\n",
    "        Y_hat = torch.empty((nsamples+ncorrupt), dtype=torch.float32).to(device)\n",
    "        Y_hat[:nsamples] = score_distmult_bc((train_triples[:, 0],\n",
    "                                              train_triples[:, 1],\n",
    "                                              train_triples[:, 2]),\n",
    "                                             node_embeddings,\n",
    "                                             edge_embeddings).to(device)\n",
    "\n",
    "        Y_hat[-ncorrupt:] = score_distmult_bc((corrupted_data[:, 0],\n",
    "                                               corrupted_data[:, 1],\n",
    "                                               corrupted_data[:, 2]),\n",
    "                                              node_embeddings,\n",
    "                                              edge_embeddings).to(device)\n",
    "\n",
    "        loss = criterion(Y_hat, Y)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(rgcn.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        # validate\n",
    "        rgcn.eval()\n",
    "        with torch.no_grad():\n",
    "            node_embeddings = rgcn(features) \n",
    "            edge_embeddings = rgcn.w_relation\n",
    "            ranks = compute_ranks_fast(val_triples, node_embeddings, edge_embeddings, batch_size=16)\n",
    "\n",
    "            mrr_raw = torch.mean(1.0 / ranks.float()).item()\n",
    "\n",
    "            hits_at_k = dict()\n",
    "            for k in [1, 3, 10, 100]:\n",
    "                hits_at_k[k] = float(torch.mean((ranks <= k).float())) # % of dataset which made it to top k positions\n",
    "                \n",
    "        if e> 500:\n",
    "            if best_score < 0:\n",
    "                best_score = mrr_raw\n",
    "                model_dict = OrderedDict()\n",
    "                for k, v in rgcn.state_dict().items():\n",
    "                    if k not in ['hor_graph','ver_graph']:\n",
    "                        model_dict[k] = v \n",
    "\n",
    "                best_state = model_dict\n",
    "            if mrr_raw <= best_score - delta: # if mrr is lower than before\n",
    "                patience_left -= 1\n",
    "            else:\n",
    "                best_score = mrr_raw\n",
    "                model_dict = OrderedDict()\n",
    "                for k, v in rgcn.state_dict().items():\n",
    "                    if k not in ['hor_graph','ver_graph']: # cannot save sparse tensors\n",
    "                        model_dict[k] = v \n",
    "\n",
    "                best_state = model_dict\n",
    "                patience_left = patience\n",
    "            if patience_left <= 0:\n",
    "                torch.save(best_state, f'{model_path}/lp_{best_score:.2}.pt')\n",
    "                print(\"Early stopping after no improvement for {} epoch\".format(patience))\n",
    "                break\n",
    "\n",
    "\n",
    "        # clear gpu cache\n",
    "        if device == torch.device('cuda'):\n",
    "            del node_embeddings\n",
    "            del edge_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if e%50==0:\n",
    "            epochs_list.append(e)\n",
    "            train_loss_list.append(loss.cpu().detach().numpy())\n",
    "            valid_MRR_list.append(mrr_raw)\n",
    "            for k,v in hits_at_k.items():\n",
    "                if int(k) == 1:\n",
    "                    hits1_list.append(v)\n",
    "                elif int(k) == 3:\n",
    "                    hits3_list.append(v)\n",
    "                elif int(k) == 10:\n",
    "                    hits10_list.append(v)\n",
    "                elif int(k) == 100:\n",
    "                    hits100_list.append(v)\n",
    "            print(\"{:04d} \".format(e) \\\n",
    "                         + \"| train loss {:.4f} \".format(loss)\n",
    "                         + \"| valid MRR (raw) {:.4f} \".format(mrr_raw)\n",
    "                         + \"/ \" + \" / \".join([\"H@{} {:.4f}\".format(k,v)\n",
    "                                             for k,v in hits_at_k.items()]))\n",
    "            \n",
    "    results_dict = {'epochs':epochs_list,'train_loss':train_loss_list, 'valid_MRR': valid_MRR_list, 'hits1':hits1_list, 'hits3':hits3_list, 'hits10':hits10_list, 'hits100':hits100_list}\n",
    "    df = pd.DataFrame(data=results_dict)\n",
    "    df.to_csv(os.path.join(results_path, \"lp_train_val_logs.csv\"), index=False)\n",
    "    \n",
    "    result = test_link_pred(config_yaml, emb, best_state, features, test_triples, rgcn, device, results_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ec2a60-6d9e-42bf-a98a-dd3afee5b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_link_pred(config_yaml, emb, best_state, features, test_triples, rgcn, device, results_path):\n",
    "    bases = config_yaml[\"rgcn\"][\"bases\"]\n",
    "    \n",
    "    features = torch.cat([trainable], dim=0)\n",
    "    rgcn = RGCN(data.triples.long().to(device), n=data.num_entities, r=data.num_relations, insize=emb, hidden=emb, numcls=data.num_classes, device = device, link_prediction = False, bases=bases)\n",
    "    rgcn.to(device)\n",
    "    rgcn.load_state_dict(best_state, strict=False)\n",
    "    \n",
    "    rgcn.eval()\n",
    "    mrr = 0.0\n",
    "    hits_at_k = {1: 0.0, 3: 0.0, 10: 0.0, 100:0.0}\n",
    "    ranks = None\n",
    "    with torch.no_grad():\n",
    "        node_embeddings = rgcn(features) \n",
    "        edge_embeddings = rgcn.w_relation\n",
    "\n",
    "        ranks = compute_ranks_fast(test_triples, node_embeddings, edge_embeddings, batch_size=16)\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float()).item()\n",
    "        for k in [1, 3, 10, 100]:\n",
    "            hits_at_k[k] = float(torch.mean((ranks <= k).float()))\n",
    "\n",
    "    rank_type = \"raw\"\n",
    "    print(\"Performance on test set: MRR ({}) {:.4f}\".format(rank_type, mrr) \n",
    "          + \" / \" + \" / \".join([\"H@{} {:.4f}\".format(k,v) for k,v in hits_at_k.items()]))\n",
    "\n",
    "    test_results_dict = {'test_MRR': mrr, 'hits1':hits_at_k[1], 'hits3':hits_at_k[3], 'hits10':hits_at_k[10], 'hits100':hits_at_k[100]}\n",
    "    with open(os.path.join(results_path, \"lp_test_logs.csv\"), 'w') as file:\n",
    "        file.write(json.dumps(test_results_dict)) # use `json.loads` to do the reverse\n",
    "\n",
    "    return (mrr, hits_at_k, ranks.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf762b9-1ac7-4e78-b9a7-c9a549b0867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/ongz0070/.local/share/virtualenvs/03_Assignments-a3lR10wD/lib/python3.7/site-packages/ipykernel_launcher.py:6: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPARATION\n",
      "PREPARE EMBEDDINGS\n",
      "Initializing embedding for datatype hasAPOEA1.\n",
      "Initializing embedding for datatype hasCDGLOBAL.\n",
      "Initializing embedding for datatype hasGDTOTAL.\n",
      "Computing embeddings for images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 880/880 [30:40<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding for datatype hasMMSCORE.\n",
      "Initializing embedding for datatype hasName.\n",
      "Initializing embedding for datatype hasSubjectAge.\n",
      "Initializing embedding for datatype hasSubjectSex.\n",
      "Initializing embedding for datatype hasWeightKg.\n",
      "PEFORM TASK\n",
      "epoch 00: loss 1.9e+01, train acc 0.69, \t val_loss 2.7e+01, withheld acc 0.53 \t (0.012019s)\n",
      "epoch 01: loss 2.8, train acc 0.67, \t val_loss 4.8, withheld acc 0.53 \t (0.0113s)\n",
      "epoch 02: loss 2.9e+01, train acc 0.31, \t val_loss 2.3e+01, withheld acc 0.44 \t (0.010592s)\n",
      "epoch 03: loss 2.3e+01, train acc 0.32, \t val_loss 1.9e+01, withheld acc 0.44 \t (0.010193s)\n",
      "epoch 04: loss 8.3, train acc 0.32, \t val_loss 7.3, withheld acc 0.42 \t (0.0098672s)\n",
      "epoch 05: loss 4.9, train acc 0.69, \t val_loss 7.9, withheld acc 0.53 \t (0.0097623s)\n",
      "epoch 06: loss 1e+01, train acc 0.69, \t val_loss 1.5e+01, withheld acc 0.53 \t (0.0097127s)\n",
      "epoch 07: loss 1.3e+01, train acc 0.69, \t val_loss 1.9e+01, withheld acc 0.53 \t (0.0096254s)\n",
      "epoch 08: loss 1.4e+01, train acc 0.69, \t val_loss 2e+01, withheld acc 0.53 \t (0.0095274s)\n",
      "epoch 09: loss 1.4e+01, train acc 0.69, \t val_loss 2e+01, withheld acc 0.53 \t (0.0093834s)\n",
      "epoch 10: loss 1.2e+01, train acc 0.69, \t val_loss 1.7e+01, withheld acc 0.53 \t (0.0095932s)\n",
      "epoch 11: loss 9.3, train acc 0.69, \t val_loss 1.4e+01, withheld acc 0.53 \t (0.0093656s)\n",
      "epoch 12: loss 5.9, train acc 0.69, \t val_loss 9.0, withheld acc 0.53 \t (0.0093455s)\n",
      "epoch 13: loss 2.1, train acc 0.67, \t val_loss 3.7, withheld acc 0.53 \t (0.0091722s)\n",
      "epoch 14: loss 5.2, train acc 0.34, \t val_loss 4.7, withheld acc 0.42 \t (0.0094094s)\n",
      "epoch 15: loss 8.6, train acc 0.32, \t val_loss 7.4, withheld acc 0.44 \t (0.0092413s)\n",
      "epoch 16: loss 7.1, train acc 0.32, \t val_loss 6.2, withheld acc 0.44 \t (0.0091672s)\n",
      "epoch 17: loss 2.4, train acc 0.42, \t val_loss 2.6, withheld acc 0.42 \t (0.0090075s)\n",
      "epoch 18: loss 2.3, train acc 0.68, \t val_loss 4.0, withheld acc 0.51 \t (0.0091407s)\n",
      "epoch 19: loss 4.4, train acc 0.69, \t val_loss 6.7, withheld acc 0.53 \t (0.0091174s)\n",
      "epoch 20: loss 5.4, train acc 0.69, \t val_loss 8.0, withheld acc 0.53 \t (0.0092311s)\n",
      "epoch 21: loss 5.5, train acc 0.69, \t val_loss 8.2, withheld acc 0.53 \t (0.0095234s)\n",
      "epoch 22: loss 4.9, train acc 0.69, \t val_loss 7.4, withheld acc 0.53 \t (0.0092206s)\n",
      "epoch 23: loss 3.8, train acc 0.69, \t val_loss 5.8, withheld acc 0.53 \t (0.009306s)\n",
      "epoch 24: loss 2.2, train acc 0.69, \t val_loss 3.6, withheld acc 0.53 \t (0.0092497s)\n",
      "epoch 25: loss 0.99, train acc 0.53, \t val_loss 1.6, withheld acc 0.51 \t (0.0093236s)\n",
      "epoch 26: loss 3.2, train acc 0.32, \t val_loss 3.1, withheld acc 0.44 \t (0.0091944s)\n",
      "epoch 27: loss 3.5, train acc 0.32, \t val_loss 3.4, withheld acc 0.44 \t (0.0092604s)\n",
      "epoch 28: loss 1.5, train acc 0.44, \t val_loss 1.8, withheld acc 0.4 \t (0.0090578s)\n",
      "epoch 29: loss 1.2, train acc 0.69, \t val_loss 2.2, withheld acc 0.53 \t (0.0089126s)\n",
      "epoch 30: loss 2.2, train acc 0.69, \t val_loss 3.6, withheld acc 0.53 \t (0.0088639s)\n",
      "epoch 31: loss 2.6, train acc 0.69, \t val_loss 4.2, withheld acc 0.53 \t (0.0088177s)\n",
      "epoch 32: loss 2.4, train acc 0.69, \t val_loss 3.9, withheld acc 0.53 \t (0.008971s)\n",
      "epoch 33: loss 1.7, train acc 0.69, \t val_loss 3.0, withheld acc 0.53 \t (0.0089564s)\n",
      "epoch 34: loss 0.79, train acc 0.68, \t val_loss 1.6, withheld acc 0.51 \t (0.0090778s)\n",
      "epoch 35: loss 1.5, train acc 0.42, \t val_loss 1.8, withheld acc 0.47 \t (0.0088475s)\n",
      "epoch 36: loss 2.0, train acc 0.37, \t val_loss 2.3, withheld acc 0.44 \t (0.0087104s)\n",
      "epoch 37: loss 1.1, train acc 0.51, \t val_loss 1.5, withheld acc 0.44 \t (0.0087385s)\n",
      "epoch 38: loss 0.86, train acc 0.71, \t val_loss 1.7, withheld acc 0.56 \t (0.0086341s)\n",
      "epoch 39: loss 1.4, train acc 0.69, \t val_loss 2.5, withheld acc 0.53 \t (0.0084727s)\n",
      "epoch 40: loss 1.6, train acc 0.69, \t val_loss 2.7, withheld acc 0.53 \t (0.0084116s)\n",
      "epoch 41: loss 1.2, train acc 0.69, \t val_loss 2.2, withheld acc 0.53 \t (0.0085108s)\n",
      "epoch 42: loss 0.71, train acc 0.68, \t val_loss 1.4, withheld acc 0.56 \t (0.010224s)\n",
      "epoch 43: loss 1.1, train acc 0.54, \t val_loss 1.5, withheld acc 0.4 \t (0.009903s)\n",
      "epoch 44: loss 1.3, train acc 0.48, \t val_loss 1.7, withheld acc 0.44 \t (0.0098238s)\n",
      "epoch 45: loss 0.78, train acc 0.65, \t val_loss 1.3, withheld acc 0.49 \t (0.0098772s)\n",
      "epoch 46: loss 0.8, train acc 0.69, \t val_loss 1.5, withheld acc 0.53 \t (0.0098212s)\n",
      "epoch 47: loss 1.1, train acc 0.69, \t val_loss 2.0, withheld acc 0.53 \t (0.0096254s)\n",
      "epoch 48: loss 1.0, train acc 0.69, \t val_loss 1.9, withheld acc 0.56 \t (0.0098603s)\n",
      "epoch 49: loss 0.74, train acc 0.71, \t val_loss 1.4, withheld acc 0.53 \t (0.0094421s)\n",
      "epoch 50: loss 0.76, train acc 0.66, \t val_loss 1.3, withheld acc 0.51 \t (0.0093937s)\n",
      "epoch 51: loss 0.98, train acc 0.58, \t val_loss 1.4, withheld acc 0.47 \t (0.0091846s)\n",
      "epoch 52: loss 0.76, train acc 0.67, \t val_loss 1.3, withheld acc 0.49 \t (0.0093005s)\n",
      "epoch 53: loss 0.68, train acc 0.69, \t val_loss 1.3, withheld acc 0.58 \t (0.0093012s)\n",
      "epoch 54: loss 0.84, train acc 0.71, \t val_loss 1.6, withheld acc 0.56 \t (0.0092881s)\n",
      "epoch 55: loss 0.82, train acc 0.71, \t val_loss 1.6, withheld acc 0.56 \t (0.0088711s)\n",
      "epoch 56: loss 0.66, train acc 0.7, \t val_loss 1.3, withheld acc 0.56 \t (0.0089071s)\n",
      "epoch 57: loss 0.7, train acc 0.68, \t val_loss 1.2, withheld acc 0.51 \t (0.0089848s)\n",
      "epoch 58: loss 0.79, train acc 0.64, \t val_loss 1.3, withheld acc 0.49 \t (0.008836s)\n",
      "epoch 59: loss 0.65, train acc 0.71, \t val_loss 1.2, withheld acc 0.44 \t (0.0089641s)\n",
      "epoch 60: loss 0.66, train acc 0.71, \t val_loss 1.3, withheld acc 0.58 \t (0.0087187s)\n",
      "epoch 61: loss 0.73, train acc 0.7, \t val_loss 1.5, withheld acc 0.53 \t (0.0085661s)\n",
      "epoch 62: loss 0.67, train acc 0.72, \t val_loss 1.4, withheld acc 0.53 \t (0.0084915s)\n",
      "epoch 63: loss 0.61, train acc 0.71, \t val_loss 1.2, withheld acc 0.49 \t (0.0084925s)\n",
      "epoch 64: loss 0.68, train acc 0.68, \t val_loss 1.2, withheld acc 0.47 \t (0.0084214s)\n",
      "epoch 65: loss 0.65, train acc 0.71, \t val_loss 1.2, withheld acc 0.47 \t (0.0084984s)\n",
      "epoch 66: loss 0.6, train acc 0.71, \t val_loss 1.2, withheld acc 0.56 \t (0.0085034s)\n",
      "epoch 67: loss 0.65, train acc 0.72, \t val_loss 1.3, withheld acc 0.51 \t (0.0085113s)\n",
      "epoch 68: loss 0.64, train acc 0.72, \t val_loss 1.3, withheld acc 0.53 \t (0.0085523s)\n",
      "epoch 69: loss 0.59, train acc 0.71, \t val_loss 1.2, withheld acc 0.56 \t (0.0087376s)\n",
      "epoch 70: loss 0.62, train acc 0.73, \t val_loss 1.2, withheld acc 0.42 \t (0.008533s)\n",
      "epoch 71: loss 0.62, train acc 0.73, \t val_loss 1.2, withheld acc 0.42 \t (0.0089738s)\n",
      "epoch 72: loss 0.58, train acc 0.73, \t val_loss 1.2, withheld acc 0.51 \t (0.0093405s)\n",
      "epoch 73: loss 0.6, train acc 0.7, \t val_loss 1.3, withheld acc 0.51 \t (0.0092165s)\n",
      "epoch 74: loss 0.6, train acc 0.71, \t val_loss 1.3, withheld acc 0.51 \t (0.0092204s)\n",
      "epoch 75: loss 0.57, train acc 0.72, \t val_loss 1.2, withheld acc 0.49 \t (0.0091503s)\n",
      "epoch 76: loss 0.58, train acc 0.76, \t val_loss 1.2, withheld acc 0.44 \t (0.0090921s)\n",
      "epoch 77: loss 0.58, train acc 0.77, \t val_loss 1.2, withheld acc 0.44 \t (0.0089262s)\n",
      "epoch 78: loss 0.59, train acc 0.72, \t val_loss 1.3, withheld acc 0.51 \t (0.0088656s)\n",
      "epoch 79: loss 0.56, train acc 0.71, \t val_loss 1.2, withheld acc 0.51 \t (0.0088191s)\n",
      "epoch 80: loss 0.58, train acc 0.74, \t val_loss 1.2, withheld acc 0.47 \t (0.0088754s)\n",
      "epoch 81: loss 0.56, train acc 0.74, \t val_loss 1.2, withheld acc 0.47 \t (0.0086427s)\n",
      "epoch 82: loss 0.57, train acc 0.73, \t val_loss 1.2, withheld acc 0.53 \t (0.0085917s)\n",
      "epoch 83: loss 0.56, train acc 0.72, \t val_loss 1.2, withheld acc 0.51 \t (0.0084114s)\n",
      "epoch 84: loss 0.55, train acc 0.75, \t val_loss 1.1, withheld acc 0.47 \t (0.0085938s)\n",
      "epoch 85: loss 0.55, train acc 0.75, \t val_loss 1.1, withheld acc 0.44 \t (0.013285s)\n",
      "epoch 86: loss 0.55, train acc 0.73, \t val_loss 1.2, withheld acc 0.49 \t (0.0093513s)\n",
      "epoch 87: loss 0.55, train acc 0.73, \t val_loss 1.2, withheld acc 0.53 \t (0.0093379s)\n",
      "epoch 88: loss 0.54, train acc 0.72, \t val_loss 1.1, withheld acc 0.49 \t (0.0092623s)\n",
      "epoch 89: loss 0.55, train acc 0.73, \t val_loss 1.1, withheld acc 0.51 \t (0.0093522s)\n",
      "epoch 90: loss 0.54, train acc 0.73, \t val_loss 1.1, withheld acc 0.47 \t (0.0090349s)\n",
      "epoch 91: loss 0.54, train acc 0.73, \t val_loss 1.2, withheld acc 0.49 \t (0.0092437s)\n",
      "epoch 92: loss 0.54, train acc 0.73, \t val_loss 1.2, withheld acc 0.49 \t (0.0090942s)\n",
      "epoch 93: loss 0.53, train acc 0.74, \t val_loss 1.1, withheld acc 0.49 \t (0.0088804s)\n",
      "epoch 94: loss 0.53, train acc 0.73, \t val_loss 1.1, withheld acc 0.49 \t (0.0089486s)\n",
      "epoch 95: loss 0.53, train acc 0.72, \t val_loss 1.1, withheld acc 0.47 \t (0.0088563s)\n",
      "epoch 96: loss 0.53, train acc 0.73, \t val_loss 1.2, withheld acc 0.49 \t (0.0088456s)\n",
      "epoch 97: loss 0.53, train acc 0.73, \t val_loss 1.1, withheld acc 0.47 \t (0.0085449s)\n",
      "epoch 98: loss 0.52, train acc 0.73, \t val_loss 1.1, withheld acc 0.51 \t (0.0087686s)\n",
      "epoch 99: loss 0.52, train acc 0.74, \t val_loss 1.1, withheld acc 0.53 \t (0.0085416s)\n",
      "epoch 100: loss 0.52, train acc 0.73, \t val_loss 1.1, withheld acc 0.47 \t (0.0085227s)\n",
      "epoch 101: loss 0.52, train acc 0.74, \t val_loss 1.1, withheld acc 0.47 \t (0.0083442s)\n",
      "epoch 102: loss 0.52, train acc 0.73, \t val_loss 1.1, withheld acc 0.47 \t (0.008219s)\n",
      "epoch 103: loss 0.52, train acc 0.74, \t val_loss 1.1, withheld acc 0.56 \t (0.008224s)\n",
      "epoch 104: loss 0.51, train acc 0.74, \t val_loss 1.1, withheld acc 0.49 \t (0.0081494s)\n",
      "epoch 105: loss 0.51, train acc 0.74, \t val_loss 1.1, withheld acc 0.47 \t (0.0080569s)\n",
      "epoch 106: loss 0.51, train acc 0.74, \t val_loss 1.1, withheld acc 0.47 \t (0.0081241s)\n",
      "epoch 107: loss 0.51, train acc 0.75, \t val_loss 1.1, withheld acc 0.49 \t (0.007782s)\n",
      "epoch 108: loss 0.51, train acc 0.76, \t val_loss 1.1, withheld acc 0.51 \t (0.0078354s)\n",
      "epoch 109: loss 0.51, train acc 0.76, \t val_loss 1.1, withheld acc 0.49 \t (0.0077386s)\n",
      "epoch 110: loss 0.51, train acc 0.75, \t val_loss 1.1, withheld acc 0.47 \t (0.0078516s)\n",
      "epoch 111: loss 0.5, train acc 0.76, \t val_loss 1.1, withheld acc 0.47 \t (0.0078027s)\n",
      "epoch 112: loss 0.5, train acc 0.76, \t val_loss 1.1, withheld acc 0.51 \t (0.0078752s)\n",
      "epoch 113: loss 0.5, train acc 0.76, \t val_loss 1.1, withheld acc 0.49 \t (0.0079982s)\n",
      "epoch 114: loss 0.5, train acc 0.77, \t val_loss 1.1, withheld acc 0.49 \t (0.0080781s)\n",
      "epoch 115: loss 0.5, train acc 0.76, \t val_loss 1.1, withheld acc 0.49 \t (0.0081284s)\n",
      "epoch 116: loss 0.5, train acc 0.77, \t val_loss 1.1, withheld acc 0.51 \t (0.0080369s)\n",
      "epoch 117: loss 0.5, train acc 0.76, \t val_loss 1.1, withheld acc 0.51 \t (0.0080066s)\n",
      "epoch 118: loss 0.49, train acc 0.77, \t val_loss 1.1, withheld acc 0.51 \t (0.0081029s)\n",
      "epoch 119: loss 0.49, train acc 0.78, \t val_loss 1.1, withheld acc 0.49 \t (0.0079999s)\n",
      "epoch 120: loss 0.49, train acc 0.78, \t val_loss 1.1, withheld acc 0.51 \t (0.0081034s)\n",
      "epoch 121: loss 0.49, train acc 0.78, \t val_loss 1.1, withheld acc 0.51 \t (0.0079513s)\n",
      "epoch 122: loss 0.49, train acc 0.78, \t val_loss 1.1, withheld acc 0.51 \t (0.0079014s)\n",
      "epoch 123: loss 0.49, train acc 0.78, \t val_loss 1.1, withheld acc 0.51 \t (0.0079112s)\n",
      "epoch 124: loss 0.49, train acc 0.78, \t val_loss 1.1, withheld acc 0.51 \t (0.0080876s)\n",
      "epoch 125: loss 0.48, train acc 0.78, \t val_loss 1.1, withheld acc 0.51 \t (0.0079792s)\n",
      "epoch 126: loss 0.48, train acc 0.78, \t val_loss 1.1, withheld acc 0.51 \t (0.0079372s)\n",
      "epoch 127: loss 0.48, train acc 0.78, \t val_loss 1.1, withheld acc 0.49 \t (0.0078957s)\n",
      "epoch 128: loss 0.48, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0079298s)\n",
      "epoch 129: loss 0.48, train acc 0.78, \t val_loss 1.1, withheld acc 0.51 \t (0.0077469s)\n",
      "epoch 130: loss 0.48, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0078163s)\n",
      "epoch 131: loss 0.48, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.00792s)\n",
      "epoch 132: loss 0.48, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0076933s)\n",
      "epoch 133: loss 0.47, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0076802s)\n",
      "epoch 134: loss 0.47, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0078852s)\n",
      "epoch 135: loss 0.47, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0078077s)\n",
      "epoch 136: loss 0.47, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0077798s)\n",
      "epoch 137: loss 0.47, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0089958s)\n",
      "epoch 138: loss 0.47, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0094476s)\n",
      "epoch 139: loss 0.47, train acc 0.8, \t val_loss 1.1, withheld acc 0.51 \t (0.008707s)\n",
      "epoch 140: loss 0.46, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0084777s)\n",
      "epoch 141: loss 0.46, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0083878s)\n",
      "epoch 142: loss 0.46, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0086169s)\n",
      "epoch 143: loss 0.46, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.00846s)\n",
      "epoch 144: loss 0.46, train acc 0.79, \t val_loss 1.1, withheld acc 0.51 \t (0.0085771s)\n",
      "epoch 145: loss 0.46, train acc 0.8, \t val_loss 1.1, withheld acc 0.51 \t (0.0085528s)\n",
      "epoch 146: loss 0.46, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0082192s)\n",
      "epoch 147: loss 0.46, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0082352s)\n",
      "epoch 148: loss 0.46, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0080397s)\n",
      "epoch 149: loss 0.45, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0083213s)\n",
      "epoch 150: loss 0.45, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0082259s)\n",
      "epoch 151: loss 0.45, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0080268s)\n",
      "epoch 152: loss 0.45, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.007951s)\n",
      "epoch 153: loss 0.45, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0082855s)\n",
      "epoch 154: loss 0.45, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0077634s)\n",
      "epoch 155: loss 0.45, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0080142s)\n",
      "epoch 156: loss 0.45, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0078893s)\n",
      "epoch 157: loss 0.45, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0078025s)\n",
      "epoch 158: loss 0.44, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0078723s)\n",
      "epoch 159: loss 0.44, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0077505s)\n",
      "epoch 160: loss 0.44, train acc 0.81, \t val_loss 1.1, withheld acc 0.49 \t (0.0080888s)\n",
      "epoch 161: loss 0.44, train acc 0.82, \t val_loss 1.1, withheld acc 0.49 \t (0.0083263s)\n",
      "epoch 162: loss 0.44, train acc 0.82, \t val_loss 1.1, withheld acc 0.49 \t (0.009172s)\n",
      "epoch 163: loss 0.44, train acc 0.82, \t val_loss 1.1, withheld acc 0.51 \t (0.0091517s)\n",
      "epoch 164: loss 0.44, train acc 0.82, \t val_loss 1.1, withheld acc 0.49 \t (0.009017s)\n",
      "epoch 165: loss 0.44, train acc 0.82, \t val_loss 1.1, withheld acc 0.49 \t (0.0091586s)\n",
      "epoch 166: loss 0.43, train acc 0.82, \t val_loss 1.1, withheld acc 0.49 \t (0.00893s)\n",
      "epoch 167: loss 0.43, train acc 0.82, \t val_loss 1.1, withheld acc 0.51 \t (0.0088556s)\n",
      "epoch 168: loss 0.43, train acc 0.82, \t val_loss 1.1, withheld acc 0.49 \t (0.0096517s)\n",
      "epoch 169: loss 0.43, train acc 0.82, \t val_loss 1.1, withheld acc 0.49 \t (0.0084863s)\n",
      "Early stopping after no improvement for 20 epoch\n"
     ]
    }
   ],
   "source": [
    "class Args():\n",
    "    config = \"config.yaml\"\n",
    "\n",
    "args = Args()\n",
    "with open(args.config) as file:\n",
    "    config_yaml = yaml.load(file)\n",
    "    \n",
    "    print(\"DATA PREPARATION\")\n",
    "    prune = config_yaml[\"data_prep\"][\"prune\"]\n",
    "    data_path = config_yaml[\"data_prep\"]['data_path']\n",
    "    final = config_yaml[\"data_prep\"][\"final\"]\n",
    "    prune_dist = config_yaml[\"data_prep\"][\"prune_dist\"]\n",
    "    device = config_yaml[\"enivron\"][\"device\"]\n",
    "    emb = config_yaml[\"data_prep\"][\"emb\"]\n",
    "    \n",
    "    data = load.Data(data_path, final=final, use_torch=True)\n",
    "    if prune_dist is not None:\n",
    "        data = load.prune(data, n=prune_dist)\n",
    "    data = load.group(data)\n",
    "    \n",
    "    print(\"PREPARE EMBEDDINGS\")\n",
    "    use_saved_embeddings = config_yaml[\"embeddings\"][\"use_saved_embeddings\"]\n",
    "    imagebatch = config_yaml[\"embeddings\"][\"imagebatch\"]\n",
    "    stringbatch = config_yaml[\"embeddings\"][\"stringbatch\"]\n",
    "    sample_size = config_yaml[\"embeddings\"][\"sample_size\"]\n",
    "    sample_duration = config_yaml[\"embeddings\"][\"sample_duration\"]\n",
    "    embedding_path = config_yaml[\"embeddings\"][\"embedding_path\"]\n",
    "    cnn_type = config_yaml[\"embeddings\"][\"cnn_type\"]\n",
    "\n",
    "    if not use_saved_embeddings:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            embeddings = [] # number of nodes\n",
    "\n",
    "            for datatype in data.datatypes():\n",
    "                # categorical variables - the relations are just for me to pick out categorical variables without needing to label them beforehand\n",
    "                if datatype in ['hasName','hasSubjectSex','hasAPOEA1','hasAPOEA2','hasMMSCORE','hasGDTOTAL','hasCDGLOBAL','hasMoCa','hasESS','hasUPSIT','hasSTAI']:\n",
    "                    print(f'Initializing embedding for datatype {datatype}.')\n",
    "                    n = len(data.get_strings(dtype=datatype)) # initialise the dictionary\n",
    "                    embedding = nn.Embedding(n, emb)\n",
    "                    embedding = embedding.to(device)\n",
    "                    # create indices for all the values to feed into embedding\n",
    "                    x = torch.LongTensor([idx for idx,i in enumerate(data.get_strings(dtype=datatype))])\n",
    "                    x = x.to(device)\n",
    "                    categorical_embedding = embedding(x)\n",
    "\n",
    "                    embeddings.append(categorical_embedding.float())\n",
    "\n",
    "                #numerical variables\n",
    "                if datatype in ['hasWeightKg','hasSubjectAge']:\n",
    "                    print(f'Initializing embedding for datatype {datatype}.')\n",
    "                    n = len(data.get_strings(dtype=datatype))\n",
    "                    linear = nn.Linear(1, emb)\n",
    "                    linear = linear.to(device)\n",
    "                    array = [float(x) for x in data.get_strings(dtype=datatype)]\n",
    "                    x = torch.unsqueeze(torch.FloatTensor(array), dim=1)\n",
    "                    x = x.to(device)\n",
    "                    linear_embedding = linear(x)\n",
    "\n",
    "                    embeddings.append(linear_embedding.float())\n",
    "\n",
    "                # images \n",
    "                elif datatype == 'hasImage':\n",
    "                    # squeezenet for 3d mri images\n",
    "                    print(f'Computing embeddings for images.')\n",
    "                    if cnn_type == \"squeezenet\":\n",
    "                        image_embeddings = squeeze_emb(data.get_images(), sample_size, sample_duration, device, bs=imagebatch)\n",
    "                    elif cnn_type == \"SFCNnet\":\n",
    "                        image_embeddings = sfcn_emb(data.get_images(), device, bs=imagebatch)\n",
    "                    image_embeddings = pca(image_embeddings, target_dim=emb, device=device)\n",
    "                    embeddings.append(image_embeddings.float())\n",
    "\n",
    "\n",
    "                # strings    \n",
    "                elif datatype == \"hasCONDTERM\":\n",
    "                    # embed medical conditions with bio-clinical bert\n",
    "                    print(f'Computing embeddings for datatype {datatype}.')\n",
    "                    string_embeddings = bert_emb(data.get_strings(dtype=datatype), device, bs_chars=stringbatch)\n",
    "                    string_embeddings = pca(string_embeddings, target_dim=emb, device= device) #emb is input size dimensions\n",
    "                    embeddings.append(string_embeddings.float())\n",
    "\n",
    "            # data loader clusters nodes by data type, and in order given by data._datasets\n",
    "            embeddings = torch.cat(embeddings, dim=0).to(torch.float)\n",
    "        if cnn_type == \"squeezenet\":\n",
    "            torch.save(embeddings, os.path.join(embedding_path, 'squeezenet_embeddings.pt'))\n",
    "        elif cnn_type == \"SFCNnet\":\n",
    "            torch.save(embeddings, os.path.join(embedding_path, 'sfcn_embeddings.pt'))\n",
    "            \n",
    "        # all our embeddings are trainable\n",
    "        trainable = nn.Parameter(embeddings)\n",
    "    else:\n",
    "        if cnn_type == \"squeezenet\":\n",
    "            embeddings = torch.load(os.path.join(embedding_path, 'squeezenet_embeddings.pt'))\n",
    "        elif cnn_type == \"SFCNnet\":\n",
    "            embeddings = torch.load(os.path.join(embedding_path, 'sfcn_embeddings.pt'))\n",
    "        # all our embeddings are trainable\n",
    "        trainable = nn.Parameter(embeddings)\n",
    "        \n",
    "    print(\"PEFORM TASK\")\n",
    "    \n",
    "    link_prediction = config_yaml[\"general\"][\"link_prediction\"]\n",
    "    node_classification = config_yaml[\"general\"][\"node_classification\"]\n",
    "    \n",
    "    if link_prediction == True:\n",
    "        link_prediction_task(config_yaml, data, emb, trainable, device)\n",
    "    if node_classification == True:\n",
    "        node_classification_task(config_yaml, data, emb, trainable, device)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472a367-e79f-428d-9670-22eee2fa0cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
